---
title: "GARCH(1,1) & Portfolio Optimization"
author: "Lorenzo Spinazzi"
date: "2024-06-17"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Packages Used:
```{r, warning=FALSE, message=FALSE}
library(quantmod)
library(MSGARCH)
library(e1071)
library(xts)
library(caret)
```

# Introduction

The MS-GARCH package in R (Ardia et al., 2022) provides a range of methods for working with various conditional volatility models. In this example, we will focus on the simple Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model, specifically the GARCH(1,1) model. 

The objective is to accurately forecast the volatility term structure of the SPDR S&P 500 ETF trust (SPY). By training the model on SPY return data, we can utilize the package's features to predict future volatility. The prediction accuracy is evaluated by comparing the predicted values to the actual realized volatility, which is proxied by the Chicago Board Options Exchange Volatility Index (VIX), commonly known as the "fear gauge."

It is important to note that, like many models, the GARCH model makes certain return distribution assumptions. In this example, we compare the performance of different underlying model distributions to determine which one most frequently yields the most accurate volatility forecasts. 

Additionally, we are interested in the economic implications of using conditional volatility models for portfolio rebalancing. In this example, we create a two-asset portfolio comprised of a "risky" asset (SPY) and a "risk-free" asset (IEF: iShares 7-10 Year Treasury Bond ETF). The expectation is to achieve enhanced risk-adjusted returns by dynamically rebalancing the weight allocation to the risky asset (and consequently the risk-free asset, since we are fully invested) based on the predictions of future volatility term structure.

# Data Download

```{r, warning=FALSE}
# SPY, VIX, and IEF data download
tics <- c("SPY", "^VIX", "IEF")
getSymbols(tics)

# SPY daily log returns
SPY.r <- na.omit(log(SPY$SPY.Adjusted/lag(SPY$SPY.Adjusted)))

# VIX daily
VIX <- VIX$VIX.Adjusted

# IEF daily log returns
IEF.r <- na.omit(log(IEF$IEF.Adjusted/lag(IEF$IEF.Adjusted)))

# xts with all 3 time series
data <- na.omit(merge(SPY.r, IEF.r, VIX$VIX.Adjusted))
colnames(data) <- c("SPY", "IEF", "VIX")

# Inspect data
head(data)
tail(data)
```

# GARCH(1,1) Model

Even though we have the MSGARCH package that handles the heavy lifting, it's still imperative to understand how the GARCH(1,1) model functions. Under the general GARCH framework, stock returns follow a conditional normal distribution:

$$R_{d+1} = \mu_{d+1} +\epsilon_{d+1} $$

where:

$$\epsilon_{d+1} = \sigma_{d+1} \nu_{d+1}$$
$$\nu_{d+1} | \mathcal{F}_d \sim N(0,1)$$

The model represents the return variance one time step ahead ($\sigma^2_{d+1}$) conditioned on the information available "today" as:

$$\sigma^2_{d+1} = \omega + \alpha \epsilon^2_d + \beta \sigma^2_d$$

This formula describes the evolution of the variance of returns, accounting for three components: $\alpha \epsilon^2_d$, $\beta \sigma^2_d$, and $\omega$.

The $\epsilon^2_d$ component in the formula can be considered the short-term memory component, reflecting the recent shock in returns to the mean return process. Essentially, referring back to the formula for $R_{d+1}$, a return consists of a long-term average ($\mu_{d+1}$) and some recent "innovation" ($\epsilon_{d+1}$). Thus the next time step's return variance is influenced by the previous time step's shock in return.

On the other hand, the $\beta\sigma^2_d$ component represents the long-term memory component. Given the recursive nature of the GARCH(1,1) model, $\sigma^2_{d+1}$ depends on $\sigma^2_{d}$ which depended on $\sigma^2_{d-1}$ and so forth. Unlike $\epsilon^2_d$, this component captures the long-term memory of the conditional volatility process. 

The $\alpha$ and $\beta$ are simply scaling factors that determine how much importance to assign to each component when building a volatility term structure.

The last component is $\omega$ which ensures that the volatility process is mean-reverting. GARCH models pull volatility back to a long-term average equilibrium point, $\sigma^2$. Using unconditional expectations and the law of iterated expectations, we can derive from the previous expression:

$$\sigma^2 = \frac{\omega}{1 - \alpha - \beta} $$

Note that from this expression, we see that the condition $\alpha + \beta < 1$ must hold to ensure that the return variance is positive and the process is stationary.

When forecasting the next period's return variance given all information available at the previous period, i.e., $\mathbb{E}[\sigma^2_{d+1}]$, the following equation is used:

$$\mathbb{E}[\sigma^2_{d+1}] =\sigma^2 + \alpha(\epsilon^2_d - \sigma^2) + \beta(\sigma^2_d - \sigma^2)$$
This shows that the process will always be pulled back to the $\sigma^2$ equilibrium point. The speed of reversion is determined by the value of $\gamma$ which is equal to $\alpha + \beta$, where $\gamma \in (0,1)$. A larger $\gamma$ results in a longer time for volatility term structure to revert to the equilibrium value, and the opposite is true for a smaller $\gamma$. 

Fortunately, the MSGARCH package simplifies model parameter tuning and volatility forecasting. By inputting SPY return data, the package tunes the necessary model parameters. Using these specifications, it then forecasts the next periodâ€™s volatility. 

In the following code, this process is demonstrated: a GARCH(1,1) model is tuned based on 6 months of SPY return data. Using the tuned model, a forecast is made for the next day's volatility. The 6-month GARCH model tune rolling window is repeated in a loop always forecasting one period/day ahead.

```{r}
GARCH <- function(data, Gdist, n){
  
  # Start index (ensuring approximately 6 months of model training data)
  index <- 126
  
  # Initialzing vector of forecasts
  forecasts <- c()
  
  # Loop predicting next day's volatility given previous 126 days (6 months)
  for(i in index:n){
    
    # Daily returns in model training interval
    returns <- data[(index - 125):index]
    
    # Model specifications (Standard GARCH(1,1) with user specified innovation 
    # distribution)
    spec <- CreateSpec(variance.spec = list(model = "sGARCH"),
                       distribution.spec = list(distribution = Gdist))
    
    # Fit the model to the data according to specifications
    fit.ml <- FitML(spec = spec, data = returns)
    
    # Predict next day's volatility
    pred <- predict(fit.ml, nahead = 1, do.return.draw = TRUE)
    
    # Add prediction to forecast vector
    forecasts <- append(forecasts, as.numeric(pred$vol))
    
    # Increment index and repeat
    index <- index + 1
  }
  
  # Function returns daily volatility forecasts
  return(forecasts)
}
```

Let's test the function and compare the volatility forecasts from the GARCH(1,1) model with the true volatility, as measured by the VIX:

```{r}
# Number of days to predict 
n <- nrow(data)

# GARCH volatility predictions (normal underlying distribution)
GARCH.norm <- GARCH(SPY.r, "norm", n)
GARCH.norm <- as.xts(GARCH.norm, order.by = index(data[126:nrow(data)]))

# True/realized volatility proxied by the VIX index
sigma.true <- VIX[126:nrow(data)] / (100 * sqrt(252))

# Merging time series
table <- na.omit(merge(GARCH.norm, sigma.true))

# Plot results
plot(as.numeric(table$VIX.Adjusted), type = "l", main = "GARCH (1,1) Volatility Prediction Accuracy",
     ylab = "SPY Volatility", ylim = c(min(table), max(table)))
lines(as.numeric(table$GARCH.norm), col = "red")
legend("topleft", legend = c("Realized Volatility", "GARCH(1,1) Forecast"), col = c("black", "red"), lty = 1)
```

We observe that the GARCH(1,1) forecasts closely trace the realized volatility. However, this raises an important question: is the normality assumption of returns valid? Let's examine the distribution of SPY returns:

```{r}
x <- seq(-0.20, 0.20, 0.001)
y <- dnorm(x, mean = mean(SPY.r), sd = sd(SPY.r))
plot(density(SPY.r), main = "SPY Return Distribution")
lines(x, y, col = "red", lty = 2)
legend("topright", legend = c("SPY Return Distribution", "Gaussian Distribution"), col = c("black", "red"), lty = c(1,2))
```

Here we see the limitations of the normality assumption of returns. In reality, returns exhibit a greater concentration around the mean, as evidenced by the larger central peak. Additionally, return distributions are skewed and display enhanced kurtosis ("fat tails"), indicating a higher probability of extreme events than predicted by a Gaussian distribution. This highlights a crucial limitation of relying on normality assumptions, particularly in the context of risk and volatility assessment. 

To address this, we will now explore GARCH(1,1) forecasts using the following distributions: skewed normal, Student's-t, and skewed Student's-t. We will compare their prediction accuracy against the standard normal distribution.

```{r}
# Skewed normal predictions
GARCH.snorm <- GARCH(SPY.r, "snorm", n)

# Student's-t predictions
GARCH.std <- GARCH(SPY.r, "std", n)

# Skewed Student's-t predictions
GARCH.sstd <- GARCH(SPY.r, "sstd", n)

# Convert to xts objects
GARCH.snorm <- as.xts(GARCH.snorm, order.by = index(data[126:nrow(data)]))
GARCH.std <- as.xts(GARCH.std, order.by = index(data[126:nrow(data)]))
GARCH.sstd <- as.xts(GARCH.sstd, order.by = index(data[126:nrow(data)]))
```

```{r}
# Table of predictions and realized volatility (RV)
table <- na.omit(merge(GARCH.norm, GARCH.snorm, GARCH.std, GARCH.sstd, sigma.true))
colnames(table) <- c("norm", "snorm", "std", "sstd", "RV")

# Plot predictions and RV
plot(as.numeric(table$RV), type = "l", main = "GARCH(1,1) Volatility Prediction Accuracy",
     ylab = "SPY Volatility", ylim = c(min(table), max(table)))
lines(as.numeric(table$norm), col = "red")
lines(as.numeric(table$snorm), col = "darkorange")
lines(as.numeric(table$std), col = "blue")
lines(as.numeric(table$sstd), col = "darkgreen")
legend("topleft", legend = c("Realized Volatility", "Normal", "Skewed Normal", "Student's-t",
                              "Skewed Student's-t"), col = c("black", "red", "darkorange",
                                               "blue", "darkgreen"), lty = 1,
                                                cex = 0.7)
```

Although the graphic doesn't provide much insight into which distribution assumption yields the most accurate forecast on average, it does show the general accuracy of all four distribution assumptions.

To gain a deeper understanding, we should examine which distribution is most accurate at each time step and tally the results to determine the most effective distribution assumption overall.

```{r}
# Absolute value of distance of predictions from RV
norm.dist <- abs(table$norm - table$RV)
snorm.dist <- abs(table$snorm - table$RV)
std.dist <- abs(table$std - table$RV)
sstd.dist <- abs(table$sstd - table$RV)
dist.table <- data.frame(norm.dist, snorm.dist, std.dist, sstd.dist)

# Categorical variable indicating which distribution assumption yielded 
# prediction closest to RV

# 1 = Normal distribution
# 2 = Skewed normal distribution
# 3 = Student's-t distribution
# 4 = Skewed Student's-t distribution

best.dist <- c()
for(i in 1:nrow(dist.table)){
  best.dist[i] <- which(dist.table[i, ] == min(dist.table[i, ]))
}

# Totals
table(best.dist)
```

To our surprise, the skewed Student's-t distribution (categorical variable 4) outperformed the other distributions in terms of forecast accuracy. This is understandable, given that the skewed Student's-t distribution accommodates the asymmetry and heavy tails often observed in real-world return data.

# Portfolio Rebalancing

As established in the introduction, the predicted volatility values outputted by the models will serve as crucial factors in weight allocation of a portfolio consisting of a risky asset (SPY) and a risk-free asset (IEF). The portfolio rebalancing in the backtesting procedure will follow the volatility-managed portfolio framework illustrated by Moreira and Muir (2017):


$$ \omega_{i, t} = \frac{c}{\hat{\sigma}^2_{i,t+1|t}}$$
where:

$$c = \frac{1}{\bigg(\frac{1}{\hat{\sigma}^2_{1,t+1|t}}\bigg) + \bigg(\frac{1}{\hat{\sigma}^2_{2,t+1|t}}\bigg) } $$

- $\omega_{i,t}$ is the weight allocated to the risky asset at time t

- c is a constant calculated based on the forecast of both asset volatilities for the next period

- $\hat{\sigma}^2_{i,t+1|t}$ represents the forecasted variance of asset i for the next period 

Note that the risk-free volatility forecast is constructed by taking the standard deviation of the previous 50 days of daily returns (i.e., a 50-day rolling window). Given that the portfolio comprises only two assets and the sum of the weights must equal 1 (for a fully invested portfolio), the weight allocated to the risk-free asset is simply the complement of the weight allocated to risky asset.

By incorporating the predicted volatility values from our best GARCH(1,1) performer (skewed Student's-t distribution), we aim to optimize the portfolio's risk-adjusted returns, measured using the Sharpe ratio.

We will use daily returns between the dates of 2024-01-01 and 2024-06-01, dynamically rebalance the portfolio based on the volatility forecasts, and then compare the portfolio's Sharpe ratio to that of a basic, static 60/40 allocation:

```{r}

# Data for portfolio rebalancing period
SPY.r.test <- SPY.r["2024-01-01/2024-06-01",]
IEF.r.test <- IEF.r["2024-01-01/2024-06-01",]
VIX.test <- VIX["2024-01-01/2024-06-01",]
vol.pred <- GARCH.sstd["2024-01-01/2024-06-01",]

# 50-day moving average standard deviation IEF volatility calculation
IEF.MA.vol <- rollapply(IEF.r, 50, sd)
IEF.MA.vol <- IEF.MA.vol["2024-01-01/2024-06-01",]
trade.data <- na.omit(merge(SPY.r.test, IEF.r.test, VIX.test, vol.pred, IEF.MA.vol))

# Portfolio rebalancing function
portfolio.rebal <- function(trade.data){
  SPY <- as.numeric(trade.data[,1])
  IEF <- as.numeric(trade.data[,2])
  VIX <- as.numeric(trade.data[,3])
  SPY.sigma <- as.numeric(trade.data[,4])
  IEF.sigma <- as.numeric(trade.data[,5])
  n <- nrow(trade.data)
  
  returns <- c()
  
  for(i in 1:(n-1)){
    c <- 1 / ((1 / SPY.sigma[i + 1]) + (1 / IEF.sigma[i + 1]))
    omega <- c / (SPY.sigma[i + 1])
    returns[i] <- omega * SPY.r[i] + ((1 - omega) * IEF.r[i])
  }
  return(returns)
}

# GARCH(1,1) Sharpe calculations
GARCH.rebal <- portfolio.rebal(trade.data)
GARCH.mu <- mean(GARCH.rebal)
GARCH.sigma <- sd(GARCH.rebal)
GARCH.sr <- GARCH.mu / GARCH.sigma

# 60/40 static Sharpe calculations
static.port <- c()
for(i in 1:nrow(trade.data)){
  static.port[i] <- (0.60 * trade.data$SPY.Adjusted[i]) + (0.40 * trade.data$IEF.Adjusted[i])
}
static.mu <- mean(static.port)
static.sigma <- sd(static.port)
static.sr <- static.mu / static.sigma

# Results
data.frame("GARCH Sharpe" = GARCH.sr, "Static Sharpe" = static.sr)
```

As we can see, integrating conditional volatility forecasting into portfolio allocation offers clear economic benefits. Stay tuned for future posts where I will explore Markov-switching GARCH models, which allow the GARCH specifications to utilize multiple distributions based on model-identified states or regimes.

# References

Ardia, D., K. Bluteau, L. Catania, K. Boudt, A. Ghalanos, B. Peterson, and D.-A. Trottier (2022). Package â€™msgarchâ€™. The Comprehensive R Archive Network.

Bollerslev, T. (1986). Generalized autoregressive conditional heteroskedasticity. Journal of Econometrics 31(03).

Moreira, A. and T. Muir (2017). Volatility-managed portfolios. The Journal of Finance 72(04).
